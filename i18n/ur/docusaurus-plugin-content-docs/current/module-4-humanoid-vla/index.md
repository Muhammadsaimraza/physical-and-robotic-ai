# Module 4: Humanoid VLA Systems

Welcome to the final and most advanced module of this course. You have built the nervous system (ROS 2), the body and world (Gazebo), and the AI brain (Isaac). Now, you will put them all together to create a **Vision-Language-Action (VLA)** system for a humanoid robot.

This module represents the frontier of modern robotics: the convergence of large language models with physical embodiment. You will learn how to give a robot the ability to understand natural language commands, to reason about the physical world, and to translate its intentions into motion.

You will learn the principles of humanoid kinematics and motion planning, and you will build a complete system that can take a voice command like "bring me the apple" and turn it into a sequence of navigation and manipulation actions.

## Learning Objectives
By the end of this module, you will be able to:

*   Understand the principles of humanoid kinematics and dynamics.
*   Use a motion planning library like MoveIt2 to plan and execute arm movements.
*   Build a "Voice-to-Action" pipeline using a speech-to-text engine and a Large Language Model (LLM).
*   Integrate navigation, perception, and manipulation into a single, cohesive system.
*   Complete a capstone project that demonstrates a voice-commanded humanoid performing a fetch task.

## Chapters
*   **Chapter 18: Humanoid Kinematics & Dynamics**
*   **Chapter 19: Motion Planning for Manipulation**
*   **Chapter 20: Conversational Robotics & VLAs**
*   **Chapter 21: Module 4 Capstone**
